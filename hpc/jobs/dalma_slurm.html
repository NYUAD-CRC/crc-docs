

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>A Detailed SLURM Guide &mdash; CRC Documentation  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Research Publications" href="../research/index.html" />
    <link rel="prev" title="Performance Considerations" href="performance.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/crc-wordmark-light.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p><span class="caption-text">HPC Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../hpc.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accounts/index.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../system/index.html">System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../storage/index.html">Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/index.html">Software</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Jobs Management</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="batch_job.html">Batch Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="interactive.html">Interactive Sessions</a></li>
<li class="toctree-l2"><a class="reference internal" href="job_array.html">Job Array</a></li>
<li class="toctree-l2"><a class="reference internal" href="parallel_job_array.html">Parallel Job Array</a></li>
<li class="toctree-l2"><a class="reference internal" href="job_submit.html">Submitting a Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="job_status.html">Checking Job Status</a></li>
<li class="toctree-l2"><a class="reference internal" href="job_cancel.html">Cancelling a Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance.html">Performance Considerations</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">A Detailed SLURM Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#slurm-partitions">SLURM: Partitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-submitting-jobs">SLURM: Submitting Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-arguments">SLURM: Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-job-dependencies">SLURM: Job Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-listing-jobs">SLURM: Listing Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">SLURM: Listing Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-job-progress">SLURM: Job Progress</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-killing-jobs">SLURM: Killing Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-tasks">SLURM: Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-node-list">SLURM: Node List</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-accounts">SLURM: Accounts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-account-limits">SLURM: Account Limits</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-account-usage">SLURM: Account Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-system-usage">SLURM: System Usage</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../research/index.html">Research Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/index.html">Help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/index.html">Dalma Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dalma_load/index.html">Dalma Load</a></li>
</ul>
<p><span class="caption-text">RCS Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../rcs/rcs.html">Research Computing Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rcs/1001n.html">1001n Research Cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rcs/kb/index.html">Knowledge Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rcs/data_science/index.html">Research Data Science Services</a></li>
</ul>
<p><span class="caption-text">Team</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../team.html">Center for Research Computing Team</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">CRC Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Jobs Management</a> &raquo;</li>
        
      <li>A Detailed SLURM Guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="a-detailed-slurm-guide">
<h1>A Detailed SLURM Guide<a class="headerlink" href="#a-detailed-slurm-guide" title="Permalink to this headline">¶</a></h1>
<section id="slurm-partitions">
<h2>SLURM: Partitions<a class="headerlink" href="#slurm-partitions" title="Permalink to this headline">¶</a></h2>
<p>A partition is a collection of nodes, they may share some attributes (CPU type, GPU, etc)</p>
<ul class="simple">
<li><p>Compute nodes may belong to multiple partitions to ensure maximum use of the system</p></li>
<li><p>Partitions may have different priorities and limits of execution and may limit who can use them</p></li>
<li><dl class="simple">
<dt>Dalma’s partition (as seen by users)</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">serial</span></code>    : Run single core and multi-threaded jobs (eg single node)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parallel</span></code>  : Run MPI jobs (eg multi-node)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bigmem</span></code>   : Run jobs on large memory systems only</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">visual</span></code>   : Run jobs on systems with a graphics card only</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>There are other partitions, but these are reserved for specific groups and research projects</p></li>
<li><p>For those who are experts in SLURM we use partitions to request GPUs, large memory, and visual instead of “constraints” as this approach gives us more flexibility for priorities and resource limits.</p></li>
</ul>
</section>
<section id="slurm-submitting-jobs">
<h2>SLURM: Submitting Jobs<a class="headerlink" href="#slurm-submitting-jobs" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>To submit a job first you write a “job script”</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH –p serial</span>
<span class="c1">#SBATCH –n 1</span>
./myprogram
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Then you submit the script in any of the following manner</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; sbatch job.sh
<span class="c1">#OR</span>
$&gt; sbatch &lt; job.sh
<span class="c1">#OR</span>
$&gt; sbatch <span class="s">&lt;&lt; EOF</span>
<span class="s">#!/bin/bash</span>
<span class="s">#SBATCH –p serial</span>
<span class="s">#SBATCH –n 1</span>
<span class="s">./myprogram</span>
<span class="s">EOF</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
</section>
<section id="slurm-arguments">
<h2>SLURM: Arguments<a class="headerlink" href="#slurm-arguments" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Arguments to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> can be put on the command line or embedded in the job script</p></li>
<li><p>Putting them in the job script is a better option as then it “documents” how to rerun your job</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH –p serial</span>
<span class="c1">#SBATCH –n 1</span>
./myprogram
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; sbatch job1.sh
</pre></div>
</div>
<p>OR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
./myprogram
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; sbatch –p serial –n <span class="m">1</span> job2.sh
</pre></div>
</div>
<p><strong>Common Job submission arguments:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-n</span></code>   Select number of tasks to run (default 1 core per task)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-C</span></code>   Select required system feature (eg avx2, sse, gpu)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-N</span></code>   Select number of nodes on which to run</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span></code>   Wallclock in hours:minutes:seconds (ex 4:00:00)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-p</span></code>   Select partition (serial, parallel, gpu, bigmem)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-o</span></code>   Output file ( with no –e option, err and out are merged to the Outfile)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-e</span></code>   Keep a separate error File</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-d</span></code>   Dependency with prior job (ex don’t start this job before job XXX terminates)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-A</span></code>   Select account (ex physics_ser, faculty_ser)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-c</span></code>   Number of cores required per task (default 1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--tasks-per-node</span> <span class="pre">Number</span></code> of tasks on each node</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mail-type=type</span></code> Notify on state change: BEGIN, END, FAIL or ALL</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mail-user=user</span></code> Who to send email notification</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mem</span></code> Maximum amount of memory per job (default is in MB, but can use GB suffix) (Note: not all memory is available to jobs, 8GB is reserved on each node for the OS) (So a 128GB node can allocate up to 120GB for jobs)</p></li>
</ul>
</section>
<section id="slurm-job-dependencies">
<h2>SLURM: Job Dependencies<a class="headerlink" href="#slurm-job-dependencies" title="Permalink to this headline">¶</a></h2>
<p>Submitting with dependencies: Useful to create workflows</p>
<ul class="simple">
<li><p>Any specific job may have to wait until any of the specified conditions are met</p></li>
<li><dl class="simple">
<dt>These conditions are set with –d type:jobid where type can be:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">after</span></code>         run after &lt;jobid&gt; has terminated</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">afterany</span></code>      if &lt;jobid&gt; is a job array run after any job in the job array has terminated</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">afterok</span></code>       run after &lt;jobid&gt; if it finished successfully</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">afternotok</span></code>    run after &lt;jobid&gt; if it failed to finish successfully</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#Wait for specific job array elements</span>
sbatch --depend<span class="o">=</span>after:123_4 my.job
sbatch --depend<span class="o">=</span>afterok:123_4:123_8 my.job2
<span class="c1">#Wait for entire job array to complete</span>
sbatch --depend<span class="o">=</span>afterany:123 my.job
<span class="c1">#Wait for entire job array to complete successfully</span>
sbatch --depend<span class="o">=</span>afterok:123 my.job
<span class="c1">#Wait for entire job array to complete and at least one task fails</span>
sbatch --depend<span class="o">=</span>afternotok:123 my.job
</pre></div>
</div>
</section>
<section id="slurm-listing-jobs">
<h2>SLURM: Listing Jobs<a class="headerlink" href="#slurm-listing-jobs" title="Permalink to this headline">¶</a></h2>
<p>Each submitted job is given a unique number
* You can list your jobs to see which ones are waiting (pending), running
* As well as how long a job has been running and on which node(s)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; squeue
JOBID           PARTITION   NAME     USER    ST  TIME    NODES   NODELIST<span class="o">(</span>REASON<span class="o">)</span>
435251_<span class="o">[</span><span class="m">1</span>-50<span class="o">]</span>   ser_std     151215_F u123    PD  <span class="m">0</span>:00      <span class="m">1</span>        <span class="o">(</span>Priority<span class="o">)</span>
435252_<span class="o">[</span><span class="m">1</span>-50<span class="o">]</span>   ser_std     151215_F u123    PD  <span class="m">0</span>:00      <span class="m">1</span>        <span class="o">(</span>Priority<span class="o">)</span>
<span class="m">435294</span>          ser_std     Merge5.s u123    PD  <span class="m">0</span>:00      <span class="m">1</span>        <span class="o">(</span>Priority<span class="o">)</span>
435235_<span class="o">[</span><span class="m">20</span>-50<span class="o">]</span>  ser_std     151215_F u123    PD  <span class="m">0</span>:00      <span class="m">1</span>        <span class="o">(</span>Priority<span class="o">)</span>
435235_19       ser_std     151215_F u123    R   <span class="m">12</span>:55     <span class="m">1</span>        compute-21-8
435235_17       ser_std     151215_F u123    R   <span class="m">47</span>:34     <span class="m">1</span>        compute-21-12
435235_15       ser_std     151215_F u123    R   <span class="m">49</span>:04     <span class="m">1</span>        compute-21-7
435235_13       ser_std     151215_F u123    R   <span class="m">50</span>:34     <span class="m">1</span>        compute-21-4
435235_11       ser_std     151215_F u123    R   <span class="m">54</span>:35     <span class="m">1</span>        compute-21-9
435235_9        ser_std     151215_F u123    R   <span class="m">56</span>:35     <span class="m">1</span>        compute-21-6
435235_7        ser_std     151215_F u123    R   <span class="m">58</span>:35     <span class="m">1</span>        compute-21-5
435235_5        ser_std     151215_F u123    R   <span class="m">59</span>:36     <span class="m">1</span>        compute-21-1
435235_3        ser_std     151215_F u123    R   <span class="m">1</span>:00:36   <span class="m">1</span>        compute-21-11
435235_1        ser_std     151215_F u123    R   <span class="m">1</span>:04:37   <span class="m">1</span>        compute-21-3
</pre></div>
</div>
</section>
<section id="id1">
<h2>SLURM: Listing Jobs<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>You can look at completed jobs using the “sacct” command</p></li>
<li><p>To look at jobs you ran since July 1, 2017</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; squeue –starttime<span class="o">=</span><span class="m">2017</span>-07-01
</pre></div>
</div>
<ul class="simple">
<li><p>You can retrieve the following informations about a job after it terminates:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>AllocCPUS       Account        AssocID          AveCPU
AveCPUFreq      AveDiskRead    AveDiskWrite     AvePages
AveRSS          AveVMSize      BlockID          Cluster
Comment         ConsumedEnergy CPUTime          CPUTimeRAW
DerivedExitCode Elapsed        Eligible         End
ExitCode        GID            Group            JobID
JobName         Layout         MaxDiskRead      MaxDiskReadNode
MaxDiskReadTask MaxDiskWrite   MaxDiskWriteNode MaxDiskWriteTask
MaxPages        MaxPagesNode   MaxPagesTask     MaxRSS
MaxRSSNode      MaxRSSTask     MaxVMSize        MaxVMSizeNode
MaxVMSizeTask   MinCPU         MinCPUNode       MinCPUTask
NCPUS           NNodes         NodeList         NTasks
Priority        Partition      QOSRAW           ReqCPUFreq
ReqCPUs         ReqMem         Reserved         ResvCPU
ResvCPURAW      Start          State            Submit
Suspended       SystemCPU      Timelimit        TotalCPU
UID             User           UserCPU          WCKey
</pre></div>
</div>
<ul class="simple">
<li><p>To retrieve specific informations about a job</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; sacct -j <span class="m">466281</span> -format<span class="o">=</span>partition,alloccpus,elapsed,state,exitcode
JobID         JobName    Partition   Account   AllocCPUS  State     ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
<span class="m">466281</span>        job3.sh     par_std   cpcm_par     <span class="m">56</span>      COMPLETED    <span class="m">0</span>:0
<span class="m">466281</span>.batch  batch                 cpcm_par     <span class="m">28</span>      COMPLETED    <span class="m">0</span>:0
<span class="m">466281</span>.0      env                   cpcm_par     <span class="m">56</span>      COMPLETED    <span class="m">0</span>:0
</pre></div>
</div>
</section>
<section id="slurm-job-progress">
<h2>SLURM: Job Progress<a class="headerlink" href="#slurm-job-progress" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>You can see your job’s progress by looking at the output and error files</p></li>
<li><p>By default output and error files are named “slurm-XXX.out” and “slurm-XXX.err” where XXX is the job id</p></li>
<li><p>“tail –f” allows you to track new output as it is produced</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; cat slurm-435563.out
$&gt; more slurm-435563.out
$&gt; tail –f slurm-435563.out
</pre></div>
</div>
</section>
<section id="slurm-killing-jobs">
<h2>SLURM: Killing Jobs<a class="headerlink" href="#slurm-killing-jobs" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Sometimes you need to kill your job when you realise it is not working as expected</p></li>
<li><p>Note that your job can be killed automatically when it reaches its maximum time/memory allocation</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$&gt; scancel <span class="m">435563</span>
</pre></div>
</div>
</section>
<section id="slurm-tasks">
<h2>SLURM: Tasks<a class="headerlink" href="#slurm-tasks" title="Permalink to this headline">¶</a></h2>
<p>In SLURM users specify how many tasks – not cores! - they need (-n). Each task by default
uses 1 core. But this can be redefined by users using the “-c” option.</p>
<p>For example <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">–n</span> <span class="pre">2</span></code> is requesting 2 cores, while <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">–c</span> <span class="pre">3</span></code> <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">–n</span> <span class="pre">2</span></code> is
requesting 6 cores.</p>
<p>On Dalma/SLURM we implement an exclusive policy on nodes being used to run parallel jobs –
eg no other jobs may run on nodes allocated for running parallel jobs.</p>
<p>When submitting parallel jobs on Dalma you need not specify the number of nodes. The
number of tasks and cpus-per-task is sufficient for SLURM to determine how many nodes to
reserve.</p>
</section>
<section id="slurm-node-list">
<h2>SLURM: Node List<a class="headerlink" href="#slurm-node-list" title="Permalink to this headline">¶</a></h2>
<p>Sometimes applications require a list of nodes where they are to run in parallel to start.
SLURM keeps the list of nodes within the environment variable <code class="docutils literal notranslate"><span class="pre">$SLURM_JOB_NODELIST</span></code>.</p>
</section>
<section id="slurm-accounts">
<h2>SLURM: Accounts<a class="headerlink" href="#slurm-accounts" title="Permalink to this headline">¶</a></h2>
<p>SLURM maintains user associations which include user, account, qos, and partition. Users
may have several associations. Moreover, accounts are hierarchical. For example, account
“physics” maybe be a sub-account of “faculty”, which may be a sub-account of “institute”, etc.
When submitting jobs users with multiple associations must explicitely list the account, qos,
partition details they wish to use.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch –p serial –a physics –q normal –u benoit job
</pre></div>
</div>
<p>Dalma specific job submission tools extend SLURM’s associations to define a <code class="docutils literal notranslate"><span class="pre">default</span></code>
association. So you only need to specify accounts is, for example, you belong to multiple
accounts – ex faculty and research-lab – and you want to execute using your non-default
account. So at most you’ll need to specify:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch –p &lt;partition&gt; -a &lt;account&gt; job
</pre></div>
</div>
<p>Moreover, accounts, partitions, qos and users may each be configured with resource usage
limits. Thus the administrators can impose limits to the number of jobs queued, jobs running,
cores usage, and run time.</p>
</section>
<section id="slurm-account-limits">
<h2>SLURM: Account Limits<a class="headerlink" href="#slurm-account-limits" title="Permalink to this headline">¶</a></h2>
<p>To see you SLURM associations (and their parents) as well as your resource usage limits use
the following Dalma specific tool:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ziaw@login-0-1 ~<span class="o">]</span>$ slurm-show-my-limits.sh
---------- -------------------- -------------------- --------- ------- ------------- ------------- ------- --------- ----------- --------------------
    User              Account            Partition GrpSubmit GrpJobs       GrpTRES       MaxTRES MaxJobs MaxSubmit     MaxWall             Par Name
    ziaw             avengers                devel                                                   <span class="m">100</span>       <span class="m">200</span>
    ziaw             avengers               nvidia                   <span class="nv">cpu</span><span class="o">=</span><span class="m">40</span>,gres/+                   <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_par              par_std                        <span class="nv">cpu</span><span class="o">=</span><span class="m">2000</span>      <span class="nv">cpu</span><span class="o">=</span><span class="m">4000</span>     <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_ser              preempt                                                   <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_ser               visual                                                   <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_ser               bigmem                          <span class="nv">cpu</span><span class="o">=</span><span class="m">32</span>                   <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_ser          preempt_std                                                   <span class="m">100</span>       <span class="m">200</span>
    ziaw         avengers_ser              ser_std                         <span class="nv">cpu</span><span class="o">=</span><span class="m">200</span>                   <span class="m">100</span>       <span class="m">200</span>
                    avengers                                                                        <span class="m">100</span>       <span class="m">200</span>                        institute
                    institute                                                                        <span class="m">100</span>       <span class="m">200</span>                            nyuad
                        nyuad                          <span class="m">20000</span>    <span class="m">1000</span>                                 <span class="m">100</span>       <span class="m">200</span>                             root
                        root
                avengers_par                                                                        <span class="m">100</span>       <span class="m">200</span>                         avengers
                    avengers                                                                        <span class="m">100</span>       <span class="m">200</span>                        institute
</pre></div>
</div>
<p><strong>In this output we see:</strong></p>
<ul class="simple">
<li><p>user <code class="docutils literal notranslate"><span class="pre">benoit</span></code> can submit up to 200 jobs on <code class="docutils literal notranslate"><span class="pre">par_std</span></code> (parallel) partition, but have at most 100 jobs running consuming a maximum of 700 cores total where each jobs is limited to a maximum of 200 cores for 12 hours</p></li>
<li><p>user <code class="docutils literal notranslate"><span class="pre">benoit</span></code> can submit up to 200 jobs on <code class="docutils literal notranslate"><span class="pre">ser_std</span></code> (serial) partition, with at most 100 jobs running using a total of up to 200 cores for up to 48 hours</p></li>
<li><p>account <code class="docutils literal notranslate"><span class="pre">avengers_par</span></code> is shared with other users and together they have a limit of 2000 cores, 200 jobs queued, and 100 jobs running (eg the sum of all cores used by running jobs using account <code class="docutils literal notranslate"><span class="pre">avengers_par</span></code> can’t exceed 2000 cores)</p></li>
<li><p>account <code class="docutils literal notranslate"><span class="pre">avengers_par</span></code> is shared with other users and together they have a limit of 200 jobs queued, and 100 jobs running</p></li>
<li><p>account <code class="docutils literal notranslate"><span class="pre">avengers</span></code> is a sub-account of <code class="docutils literal notranslate"><span class="pre">nyuad</span></code> and the sum of all parallel and serial jobs can’t exceed 200 jobs queued, 100 jobs running</p></li>
</ul>
</section>
<section id="slurm-account-usage">
<h2>SLURM: Account Usage<a class="headerlink" href="#slurm-account-usage" title="Permalink to this headline">¶</a></h2>
<p>This next Dalma specific tool allows you to see how much resources you are using. This is useful when your
job can’t run because of “group resource limit” having been reached.</p>
<p>You can view the usage with the command: <code class="docutils literal notranslate"><span class="pre">slurm_show_usage</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ziaw@login-0-1 ~<span class="o">]</span>$ slurm_show_usage -u u1234
--------------------------------------------------------------------------------
slurm_show_usage V1.0 - <span class="m">2017</span> NYUAD Proprietary Software

User line show the usage and limit <span class="k">for</span> <span class="nv">user</span><span class="o">=</span>mfm15 within each account it belong.
Account lines show the usage and limit <span class="k">for</span> all users <span class="k">in</span> that account.
The indentation on Account lines represent the level of sub-accounts
up to the present account.

--------------------------------------------------------------------------------
Usage  Limit     TYPE     Account
--------------------------------------------------------------------------------
<span class="m">672</span>      <span class="m">1400</span>  User     cpcm_par
<span class="m">2296</span>      <span class="m">5000</span>  Account  cpcm_par
<span class="m">2324</span>      <span class="m">5000</span>  Account      cpcm
<span class="m">3754</span> UNLIMITED  Account          institute
<span class="m">8103</span> UNLIMITED  Account              nyuad
--------------------------------------------------------------------------------
    <span class="m">0</span>        <span class="m">32</span>  User     cpcm_ser
    <span class="m">28</span>       <span class="m">280</span>  Account  cpcm_ser
<span class="m">2324</span>      <span class="m">5000</span>  Account      cpcm
<span class="m">3754</span> UNLIMITED  Account          institute
<span class="m">8103</span> UNLIMITED  Account              nyuad
--------------------------------------------------------------------------------
</pre></div>
</div>
<ul class="simple">
<li><p>Here user <code class="docutils literal notranslate"><span class="pre">u123</span></code> has two accounts,``cpcm_par`` and <code class="docutils literal notranslate"><span class="pre">cpcm_ser</span></code>.</p></li>
<li><p>On the <code class="docutils literal notranslate"><span class="pre">cpcm_par</span></code> (parallel partition) his limit is 1400 cores, and he’s currently using 672 cores.</p></li>
<li><p>However, other users from the same account are already using 2296 cores out of the account maximum 5000.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">cpcm_par</span></code> account is a sub-account of <code class="docutils literal notranslate"><span class="pre">cpcm</span></code>, which currently is using 2324 cores out of the 5000 permitted.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">cpcm</span></code> account is also a sub-account of <code class="docutils literal notranslate"><span class="pre">institute</span></code>. All <code class="docutils literal notranslate"><span class="pre">institute</span></code> users are presently using 3754 cores out of the total cores account limit.</p></li>
<li><p>Finally “institute” is a sub-account of <code class="docutils literal notranslate"><span class="pre">nyuad</span></code> where 8103 cores are being used.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">slurm_show_usage</span></code> tool has an option to show you which account level would prevent you to run a job.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">800</span></code> option will show which
account(s) would exceed the user or
account core limit if you were to submit a
job requiring 800 cores.</p>
<img alt="../../_images/slurm_show_usage.png" src="../../_images/slurm_show_usage.png" />
<p><strong>The ``-a`` option will show all accounts usage and limit on Dalma, as well as their current usage.</strong></p>
<p>The usage limits are defined by the
academic steering committee in order to
meet each group’s computational needs,
while allowing fairness to all groups.
The account limits are periodically revised
based on prior usage statistics and inputs
from the research groups about new
project requirements.</p>
<p>Thus, the HPC support team role is limited
to implementing the recommendations
from the steering committee and to provide
the steering committee with statistics and
other key informations that help them
define fair resource usage rules.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------------------------------
slurm_show_usage V1.0 - <span class="m">2017</span> NYUAD Proprietary Software

Account lines show the usage and limit <span class="k">for</span> all users <span class="k">in</span> that account.
The indentation on Account lines represent the level of sub-accounts
up to the present account.



--------------------------------------------------------------------------------
Usage  Limit     TYPE     Account
--------------------------------------------------------------------------------
<span class="m">131</span>       <span class="m">280</span>  Account  physics_ser
<span class="m">3099</span>      <span class="m">4000</span>  Account      physics
<span class="m">4273</span> UNLIMITED  Account          faculty
<span class="m">8093</span> UNLIMITED  Account              nyuad
--------------------------------------------------------------------------------
<span class="m">2968</span>      <span class="m">3800</span>  Account  physics_par
<span class="m">3099</span>      <span class="m">4000</span>  Account      physics
<span class="m">4273</span> UNLIMITED  Account          faculty
<span class="m">8093</span> UNLIMITED  Account              nyuad
--------------------------------------------------------------------------------
</pre></div>
</div>
</section>
<section id="slurm-system-usage">
<h2>SLURM: System Usage<a class="headerlink" href="#slurm-system-usage" title="Permalink to this headline">¶</a></h2>
<p><strong>dmap</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">dmap</span></code> tool (Dalma specific) will show you the utilization of each compute node on the cluster. The first
numbers is a shorthand for the compute node name, so <code class="docutils literal notranslate"><span class="pre">12-3</span></code> actually means <code class="docutils literal notranslate"><span class="pre">compute-12-3</span></code>. The second
numbers represent the number of cores used and total number of cores in the system.
“white” highlight shows
nodes that are down for
maintenance.
“green” means a node is
busy.
No highlight means a
node is free.</p>
<p>You can launch this tool using the command <code class="docutils literal notranslate"><span class="pre">dmap</span></code></p>
<p>You can also check the different options available using the command <code class="docutils literal notranslate"><span class="pre">dmap</span> <span class="pre">-h</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ziaw@login-0-1 ~<span class="o">]</span>$ dmap -h

    Synopsis:
        Monitor utility to display <span class="k">in</span> a compact mode, Dalma utilization as well as information about <span class="nb">jobs</span> and/or nodes

    Usage:
        dmap <span class="o">[</span> -b -l -m <span class="o">]</span> <span class="o">[</span> -u &lt;users&gt; <span class="o">]</span> <span class="o">[</span> -j &lt;jobs&gt; <span class="o">[</span> -i <span class="o">]</span>  <span class="o">]</span> <span class="o">[</span> -w &lt;seconds&gt; <span class="o">]</span> <span class="o">[</span> -c &lt;columns&gt; <span class="o">]</span> <span class="o">[</span> -x <span class="o">]</span> <span class="o">[</span>-h<span class="o">]</span>

        where:

        The following metrics can be displayed:

            -b: Show allocated resources as per the batch system <span class="o">(</span>default<span class="o">)</span>
            -l: Show actual cpu load <span class="k">in</span> the nodes
            -m: Show Mem usage <span class="k">in</span> the nodes

        and you can display the latter just <span class="k">for</span> a specific <span class="nb">set</span> of jobs/users

            -u: Comma-separated list of users
            -j: Comma-separated list of <span class="nb">jobs</span>

            -i: Print Information about all <span class="nb">jobs</span> or a specific <span class="nb">set</span> of <span class="nb">jobs</span> <span class="k">if</span> -j was specified

            -w: Wait and refresh after the specified number of seconds

            -x: Dump content <span class="k">in</span> html <span class="k">for</span> web visualization

            -c: Controls how many columns will display. Default is <span class="m">9</span> because every <span class="m">18</span> nodes are conected to the same linecard <span class="k">in</span> the switches

            -h:  Prints out this helpful message
</pre></div>
</div>
<img alt="../../_images/dmap.png" src="../../_images/dmap.png" />
<p><strong>SLURM Partition Monitor</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">slurm_partition_monitor</span></code> tool gives a glimpse on the brief stats of difefrent resources in terms of partitions.</p>
<p>You can launch this tool using the command <code class="docutils literal notranslate"><span class="pre">slurm_partition_monitor</span></code></p>
<img alt="../../_images/slurm_partition_monitor.png" src="../../_images/slurm_partition_monitor.png" />
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../research/index.html" class="btn btn-neutral float-right" title="Research Publications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="performance.html" class="btn btn-neutral float-left" title="Performance Considerations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Center for Research Computing | NYU Abu Dhabi.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>
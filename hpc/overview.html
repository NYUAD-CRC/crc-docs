

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Overview &mdash; CRC Documentation  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/crc-wordmark-light.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">RCS Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../rcs/index.html">Research Computing Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rcs/kb/index.html">Knowledge Base</a></li>
</ul>
<p class="caption"><span class="caption-text">HPC Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">High Performance Computing</a></li>
<li class="toctree-l1"><a class="reference internal" href="accounts/index.html">Accounts</a></li>
<li class="toctree-l1"><a class="reference internal" href="system/index.html">System</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage/index.html">Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="software/index.html">Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs/index.html">Jobs Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="research/index.html">Research Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="help/index.html">Help</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">CRC Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Overview</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/hpc/overview.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h1>
<p>Software
On HPC:</p>
<p>The Operating System is CentOS 7. Windows / Mac software is not supported.
No GUI. No display.
You can compile / install your own software, and/or use our Module system. For the latter, first check what applications are available.</p>
<p># Run the following commands after logging in Dalma
module avail
Then you could select the desired software to load. The following example shows how to load a self-sufficient-single-application environment for gromacs.</p>
<p># Run the following commands after logging in Dalma
module load gromacs
# or use the full module name with version
module load gromacs/5.0.4
The following example shows how to load an environment for compiling source code from scratch.</p>
<p># Run the following commands after logging in Dalma
module load gcc
# multiple modules could be loaded in one line
module load openmpi fftw3
At this point, compilers like ‘gcc’, ‘gfortran’ and ‘g++’ are available, in a sense that the paths to those executables are prepended to $PATH. Also, paths to libraries files from FFTW3 will be prepended to $LD_LIBRARY_PATH.</p>
<p>If you cannot find a certain version of the software (for example, you are looking for Python 3, but only to find Python 2 is available), try running the following command to make all modules visible first.</p>
<p># Run the following commands after logging in Dalma
module load all
module avail python
————————————— /share/apps/NYUAD/modules/ALL ——————————-
python/2.7.11 python/3.5.1
As you can see, Python 3 is available then. You could load Python 3 by loading the specific module.</p>
<p>module load python/3.5.1
At this point, you should be able to invoke the executable, e.g., ‘python’.</p>
<p>Alternatively, you can use Dalma miniconda for hassle-free, independent Python environment. Follow this page: Miniconda in Dalma</p>
<p>Running Jobs
Now it is the exciting part. With input data and software ready, you can run your computational tasks now.</p>
<p>On HPC, you don’t run it directly on the login nodes. Instead, you submit jobs on login nodes. These jobs will be queued to the system and executed eventually. Conceptually, each job is a 2-step process:</p>
<p>You request certain resources from the system. The most common resources are CPU cores.
With the assigned resources, you run your computational tasks.
There are two ways, interactive sessions or batch jobs.</p>
<p>Interactive Sessions
You could get an interactive session directly from your terminal, on compute nodes. Only short interactive jobs should be used (e.g., experimenting with new modifications to your Matlab code).</p>
<p>To start an interactive session, use srun command:</p>
<p>srun –pty -n 1 /bin/bash
Then you can run your applications on the terminal directly. E.g.,</p>
<p>[<a class="reference external" href="mailto:gh50&#37;&#52;&#48;login-0-1">gh50<span>&#64;</span>login-0-1</a> ~]$ srun –pty -n 1 /bin/bash
srun: job 775175 queued and waiting for resources
srun: job 775175 has been allocated resources
[<a class="reference external" href="mailto:gh50&#37;&#52;&#48;compute-21-1">gh50<span>&#64;</span>compute-21-1</a> ~]$
In a real scenario, the system might be exhausted with no available resources to you. You need to wait in this circumstance.</p>
<p>In this example, user gh50 requested 1 CPU core (-n 1) on login node (login-0-1). The system responded, assigned a job id (775175), queued the job and assigned 1 CPU core from one of the compute nodes (compute-21-1) to the user.</p>
<p>To exit the interactive session, type Ctrl+d, or</p>
<p>exit</p>
<p>Batch Job
Besides interactive sessions, a user can submit batch jobs to the system. For production jobs, batch jobs should be used.</p>
<p>A complete batch job workflow:</p>
<p>Write a job script, which consists of 2 parts:
Resources requirement.
Commands to be executed.
Submit the job.
Relax, have a coffee, log off if you wish. The computer will do the work.
Come back to examine the result.
Batch Job Script
A job script is a text file describing the job. As discussed, the first part tells how much resources you want. The second part is what you want to run. Choose one of the following examples to start with. If you are not sure, contact us.</p>
<p>Resources Limit
The cluster is shared among the whole university. The HPC steering committee decides each year on resources limit for each department. We at NYUAD HPC center implement these limits.</p>
<p>Typically, a user can ask for 48 hours, 700 CPU cores maximum per job.</p>
<p>If you ask for more resources than you can use, your job will stay in the queue forever. (e.g., you specify 10000 hours walltime in your job script)</p>
<p>If you have multiple jobs (which is very normal), your jobs will start either immediately if the system is free and the quotas for you and your department have not been exhausted.</p>
<p>A Job with 1 CPU Core
This is a very basic example, using only one CPU core.</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
#!/bin/bash
# Set number of tasks to run
#SBATCH –ntasks=1
# Walltime format hh:mm:ss
#SBATCH –time=00:30:00
# Output and error files
#SBATCH -o job.%J.out
#SBATCH -e job.%J.err</p>
<p># <a href="#id1"><span class="problematic" id="id2">**</span></a>** Put all #SBATCH directives above this line! <a href="#id3"><span class="problematic" id="id4">**</span></a>**
# <a href="#id5"><span class="problematic" id="id6">**</span></a>** Otherwise they will not be effective! <a href="#id7"><span class="problematic" id="id8">**</span></a><a href="#id9"><span class="problematic" id="id10">**</span></a></p>
<p># <a href="#id11"><span class="problematic" id="id12">**</span></a>** Actual commands start here <a href="#id13"><span class="problematic" id="id14">**</span></a>**
# Load modules here (safety measure)
module purge
# You may need to load gcc here .. This is application specific
# module load gcc
# Replace this with your actual command. ‘serial-hello-world’ for example
hostname
As you can see, it is a simple bash script, plus some lines on the top, starting with #SBATCH, which are the Slurm directives.</p>
<p>Those Slurm directives specify resources required. E.g., ‘–ntasks=1’ is 1 CPU core. ‘–time=00:30:00’ means the maximum walltime is 30 mins. ‘-o job.%J.out’ is redirecting the stdout, usually your screen output, to a file called ‘job.$JOBID.out’. Why? Because the system will run your job in the background, hence no display.</p>
<p>Everything under the Slurm directives is normal Linux command. This example runs ‘hostname’, which will print the hostname. In reality, you should load your desired modules, and execute whatever you want to run.</p>
<p>Multithreading Job
Multithreading enables a process to spawn multiple threads to accelerate its execution. The most common multithreading model in HPC is OpenMP. If your application supports this (not sure? contact us to find out), you could use the below example.</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
#!/bin/bash
# Set number of tasks to run
#SBATCH –ntasks=1
# Set the number of CPU cores for each task
#SBATCH –cpus-per-task=4
# Walltime format hh:mm:ss
#SBATCH –time=00:30:00
# Output and error files
#SBATCH -o job.%J.out
#SBATCH -e job.%J.err</p>
<p># <a href="#id15"><span class="problematic" id="id16">**</span></a>** Put all #SBATCH directives above this line! <a href="#id17"><span class="problematic" id="id18">**</span></a>**
# <a href="#id19"><span class="problematic" id="id20">**</span></a>** Otherwise they will not be effective! <a href="#id21"><span class="problematic" id="id22">**</span></a><a href="#id23"><span class="problematic" id="id24">**</span></a></p>
<p># <a href="#id25"><span class="problematic" id="id26">**</span></a>** Actual commands start here <a href="#id27"><span class="problematic" id="id28">**</span></a>**
# Load modules here (safety measure)
module purge
# You may need to load gcc here .. This is application specific
# module load gcc</p>
<p># If you are using OpenMP application, keep this line.
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK</p>
<p># Replace this with your actual command. In this example, you should run a multithreading supported application
hostname
Comparing to the previous examples, there are 2 extra lines:</p>
<p>‘#SBATCH –cpus-per-task=4’: this asks the system to assign 4 CPU cores per tasks. This number should be no larger than and a divisor of 28 (e.g., 2, 4, 7, 14, 28) because the majority of our nodes comes with 28 cores.
‘export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK’: this tells your applications, if OpenMP supported, to use all the CPU cores assigned to your job, by spawning an exact number of OpenMP threads.
Remember, running a job is 2 steps process: 1. Request the resources. 2. Use the resources. This example is a perfect illustration. Run with what you requested, no more, no less.</p>
<p>Pure MPI Job
Now comes the pure MPI Jobs.</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
#!/bin/bash
# Set number of tasks to run
# This number should be divisible by 28. E.g., 56, 84, 112…
#SBATCH –ntasks=56
# Walltime format hh:mm:ss
#SBATCH –time=00:30:00
# Output and error files
#SBATCH -o job.%J.out
#SBATCH -e job.%J.err</p>
<p># <a href="#id29"><span class="problematic" id="id30">**</span></a>** Put all #SBATCH directives above this line! <a href="#id31"><span class="problematic" id="id32">**</span></a>**
# <a href="#id33"><span class="problematic" id="id34">**</span></a>** Otherwise they will not be effective! <a href="#id35"><span class="problematic" id="id36">**</span></a><a href="#id37"><span class="problematic" id="id38">**</span></a></p>
<p># <a href="#id39"><span class="problematic" id="id40">**</span></a>** Actual commands start here <a href="#id41"><span class="problematic" id="id42">**</span></a>**
# Load modules here (safety measure)
module purge
# You may need to load gcc here .. This is application specific
# module load gcc
# Replace this with your actual command. ‘serial-hello-world’ for example
srun hostname
Comparing to the 1 core example, there are 2 different lines:</p>
<p>‘#SBATCH –ntasks=56’: This line requests 56 cores. This number should be divisible by 28. E.g., 56, 84, 112…
‘srun hostname’: This tells your application to run with MPI support, utilizing all CPU cores requested.
The old school ‘mpiexec’ or ‘mpirun’ are supported as well. But you need to load ‘openmpi’ module in this case.
Hybrid MPI Job
If your application support MPI + OpenMP hybrid parallelization, you could follow this example to submit a hybrid job.</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
#!/bin/bash
# Set number of tasks to run
#SBATCH –ntasks=56
# Set the number of CPU cores for each task
#SBATCH –cpus-per-task=4
# Walltime format hh:mm:ss
#SBATCH –time=00:30:00
# Output and error files
#SBATCH -o job.%J.out
#SBATCH -e job.%J.err</p>
<p># <a href="#id43"><span class="problematic" id="id44">**</span></a>** Put all #SBATCH directives above this line! <a href="#id45"><span class="problematic" id="id46">**</span></a>**
# <a href="#id47"><span class="problematic" id="id48">**</span></a>** Otherwise they will not be effective! <a href="#id49"><span class="problematic" id="id50">**</span></a><a href="#id51"><span class="problematic" id="id52">**</span></a></p>
<p># <a href="#id53"><span class="problematic" id="id54">**</span></a>** Actual commands start here <a href="#id55"><span class="problematic" id="id56">**</span></a>**
# Load modules here (safety measure)
module purge
# You may need to load gcc here .. This is application specific
# module load gcc</p>
<p># If you are using Hybrid MPI + OpenMP application, keep this line.
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK</p>
<p># Replace this with your actual command. ‘serial-hello-world’ for example
srun hostname
In this case, the number of CPU cores requested is 56 (ntasks) * 4 (cpus-per-task) = 224. This number should be divisible by 28 to use all the cores on the nodes. As in the multithreading job example, make sure ‘cpus-per-task’ is a divisor of 28.</p>
<p>Job Array
This example shows how to submit a job array, consist of 100 jobs, with environmental variable SLURM_ARRAY_TASK_ID varies from 1 to 100.</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
#!/bin/bash
# Set number of tasks to run
#SBATCH –ntasks=1
# Walltime format hh:mm:ss
#SBATCH –time=00:30:00
# Output and error files
#SBATCH -o job.%J.out
#SBATCH -e job.%J.err
#SBATCH -a 1-100</p>
<p># <a href="#id57"><span class="problematic" id="id58">**</span></a>** Put all #SBATCH directives above this line! <a href="#id59"><span class="problematic" id="id60">**</span></a>**
# <a href="#id61"><span class="problematic" id="id62">**</span></a>** Otherwise they will not be effective! <a href="#id63"><span class="problematic" id="id64">**</span></a><a href="#id65"><span class="problematic" id="id66">**</span></a></p>
<p>echo “I am running job $SLURM_ARRAY_TASK_ID”
Or you can varies SLURM_ARRAY_TASK_ID from 51 to 100.</p>
<p>1
#SBATCH -a 50-100
Or set the maximum number of simultaneously running tasks from the job array to 10.</p>
<p>1
#SBATCH -a 1-100%10
We only allow a maximum of 200 jobs in queue for any given user.</p>
<p>Submitting a Job
Once you have your job script prepared, you could use the command sbatch to submit your job.</p>
<p>sbatch &lt;jobscript&gt;
Let say if you saved your job script into a file called ‘job.sh’. Then you should run the following.</p>
<p>sbatch job.sh
After the submission, it will return the corresponding job id. E.g.,</p>
<p>[<a class="reference external" href="mailto:gh50&#37;&#52;&#48;login-0-1">gh50<span>&#64;</span>login-0-1</a> overview]$ sbatch threads-job.sh
Submitted batch job 775602
In this case, the job id is 775602. You can safely log off Dalma at this point. Once the system can accommodate your request, the script will be executed. The screen output will be saved to the files you specified in the job script.</p>
<p>Checking Job Status
Before and During Job Execution
This command shows all your current jobs.</p>
<p>squeue
Example output:</p>
<dl class="simple">
<dt>[<a class="reference external" href="mailto:gh50&#37;&#52;&#48;login-0-1">gh50<span>&#64;</span>login-0-1</a> ~]$ squeue -j 31408</dt><dd><p>JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
31408   ser_std  job1.sh     gh50  R       0:02      1 compute-21-4</p>
</dd>
</dl>
<p>It means the job with Job ID 31408, has been running (ST: R) for 2 minutes on compute-21-4.</p>
<p>For more verbose information, use scontrol show job.
scontrol show job &lt;jobid&gt;
After Job Execution
Once the job is finished, the job can’t be inspected by squeue or scontrol show job. At this point, you could inspect the job by sacct.</p>
<p>sacct -j &lt;jobid&gt;
The following commands give you extremely verbose information on a job.</p>
<p>sacct -j &lt;jobid&gt; -l</p>
<p>Canceling a Job
If you decide to end a job prematurely, use scancel</p>
<p>scancel &lt;jobid&gt;
Use with Cautions</p>
<p>To cancel all jobs from your account. Run this on Dalma terminal.</p>
<p>scancel -u &lt;NetID&gt;</p>
<p>That is. Up to this point, you should be able to run your computational tasks on Dalma. If there is any question, don’t hesitate to contact us (contacts on the right)!</p>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Center for Research Computing | NYU Abu Dhabi.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>